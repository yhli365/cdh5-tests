
准备
=============================
$ mkdir ~/test
$ ln -s ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar ~/test/hadoop-mapreduce-examples.jar
$ hadoop jar hadoop-mapreduce-examples.jar
org.apache.hadoop.examples.ExampleDriver


单词统计相关作业
=============================
--wordcount(单词计数)
$ hdfs dfs -mkdir -p examples/wordcount/input
$ hdfs dfs -put ~/hadoop/etc/hadoop/*.xml examples/wordcount/input
$ hdfs dfs -ls examples/wordcount/input
$ hadoop jar hadoop-mapreduce-examples.jar wordcount examples/wordcount/input examples/wordcount/output
$ hdfs dfs -text examples/wordcount/output/part-r-* | more

--wordmean(单词长度均值)
$ hadoop jar hadoop-mapreduce-examples.jar wordmean examples/wordcount/input examples/wordmean/output
$ hdfs dfs -text examples/wordmean/output/part-r-* | more

--wordmedian(单词长度中值)
$ hadoop jar hadoop-mapreduce-examples.jar wordmedian examples/wordcount/input examples/wordmedian/output
$ hdfs dfs -text examples/wordmedian/output/part-r-* | more

--wordstandarddeviation(单词长度标准偏差)
$ hadoop jar hadoop-mapreduce-examples.jar wordstandarddeviation examples/wordcount/input examples/wordstandarddeviation/output
$ hdfs dfs -text examples/wordstandarddeviation/output/part-r-* | more

--aggregatewordcount(聚合框架单词计数)
$ hadoop jar hadoop-mapreduce-examples.jar aggregatewordcount examples/wordcount/input examples/aggregatewordcount/output 1 textinputformat
$ hdfs dfs -text examples/aggregatewordcount/output/part-r-* | more
>>>bug：ValueAggregatorJobBase#getAggregatorDescriptors和ValueAggregatorJob#setAggregatorDescriptors中参数设置不匹配。

--aggregatewordhist(聚合框架单词直方图)
$ hadoop jar hadoop-mapreduce-examples.jar aggregatewordhist examples/wordcount/input examples/aggregatewordhist/output 1 textinputformat
$ hdfs dfs -text examples/aggregatewordhist/output/part-r-* | more
>>>bug：同上

--grep(正则查找)
$ hadoop jar hadoop-mapreduce-examples.jar grep examples/wordcount/input examples/grep/output compression
$ hdfs dfs -text examples/grep/output/part-r-* | more


数据读写相关作业
=============================
--randomwriter(MAP随机写二进制数据)
$ hadoop jar hadoop-mapreduce-examples.jar randomwriter \
  -Dmapreduce.randomwriter.mapsperhost=3 \
  -Dmapreduce.randomwriter.bytespermap=10485760 \
  -Dmapreduce.randomwriter.minkey=10 \
  -Dmapreduce.randomwriter.maxkey=100 \
  -Dmapreduce.randomwriter.minvalue=0 \
  -Dmapreduce.randomwriter.maxvalue=2000 \
  examples/randomwriter/output
$ hdfs dfs -ls examples/randomwriter/output

--randomtextwriter(MAP随机写文本数据)
$ hadoop jar hadoop-mapreduce-examples.jar randomtextwriter \
  -Dmapreduce.randomtextwriter.totalbytes=104857600 \
  -Dmapreduce.randomtextwriter.bytespermap=10485760 \
  -Dmapreduce.randomtextwriter.minwordskey=5 \
  -Dmapreduce.randomtextwriter.maxwordskey=10 \
  -Dmapreduce.randomtextwriter.minwordsvalue=20 \
  -Dmapreduce.randomtextwriter.maxwordsvalue=100 \
  examples/randomtextwriter/output
$ hdfs dfs -ls examples/randomtextwriter/output
$ hdfs dfs -text examples/randomtextwriter/output/part-m-00001 | more

--sort(排序:分区|全局)
$ hadoop jar hadoop-mapreduce-examples.jar sort \
  -r 3 \
  -inFormat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat \
  -outFormat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat \
  -outKey org.apache.hadoop.io.Text \
  -outValue org.apache.hadoop.io.Text \
  examples/randomtextwriter/output \
  examples/sort/output
$ hdfs dfs -ls examples/sort/output
$ hdfs dfs -text examples/sort/output/part-r-00001 | more
>>>bug: 参数设置为"-totalOrder 0.7 120 9999"时作业执行报错，全局预分区文件读取失败。


算法
=============================
--*pi(PI值: quasi-Monte Carlo method/蒙特卡罗算法)
$ hadoop jar hadoop-mapreduce-examples.jar pi 5 3

--*bbp(PI值: Bailey-Borwein-Plouffe/贝利－波尔温－普劳夫公式)
$ hadoop jar hadoop-mapreduce-examples.jar bbp 3 4 5 examples/bbp/output
$ hdfs dfs -ls examples/bbp/output

--*distbbp(PI值)

--*pentomino(五格拼板)

--secondarysort(二次排序)
$ vi /tmp/sort1.txt
20 21
50 51
50 52
60 51
60 53

$ vi /tmp/sort2.txt
1 2
30 9
30 6
60 52

$ hdfs dfs -mkdir -p examples/secondarysort/input
$ hdfs dfs -put /tmp/sort*.txt examples/secondarysort/input
$ hadoop jar hadoop-mapreduce-examples.jar secondarysort examples/secondarysort/input examples/secondarysort/output
$ hdfs dfs -text examples/secondarysort/output/part-r-* | more

--*sudoku(数独，九宫格游戏)

--*join(连接)

--multifilewc(小文件合并MAP)
$ hdfs dfs -ls examples/wordcount/input
$ hadoop jar hadoop-mapreduce-examples.jar multifilewc \
  -Dmapreduce.input.fileinputformat.split.maxsize=9000 \
  examples/wordcount/input \
  examples/multifilewc/output
$ hdfs dfs -text examples/multifilewc/output/part-r-* | more

--dbcount(数据库表记录数统计)
$ export HADOOP_CLASSPATH=lib/mysql-connector-java-5.1.38-bin.jar
$ hadoop jar hadoop-mapreduce-examples.jar dbcount \
  -libjars lib/mysql-connector-java-5.1.38-bin.jar \
  com.mysql.jdbc.Driver \
  'jdbc:mysql://localhost:3306/demo?useUnicode=true&characterEncoding=UTF-8&user=root&password=root'
$ mysql -uroot -proot demo -e 'SELECT COUNT(*) FROM Access'
$ mysql -uroot -proot demo -e 'SELECT SUM(pageview) FROM Pageview'


排序
=============================
--teragen(数据生成)
$ hadoop jar hadoop-mapreduce-examples.jar teragen 30k examples/teragen/output
$ hdfs dfs -ls examples/teragen/output

--terasort(数据排序)
$ hadoop jar hadoop-mapreduce-examples.jar terasort \
  -Dmapreduce.job.reduces=3 \
  examples/teragen/output \
  examples/terasort/output
$ hdfs dfs -ls examples/terasort/output

--teravalidate(结果验证)
$ hadoop jar hadoop-mapreduce-examples.jar teravalidate examples/terasort/output examples/teravalidate/output
$ hdfs dfs -ls examples/teravalidate/output
$ hdfs dfs -text examples/teravalidate/output/part-r-00000 | more
